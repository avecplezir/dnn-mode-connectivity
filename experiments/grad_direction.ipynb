{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import tabulate\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import data\n",
    "import models\n",
    "import curves\n",
    "import utils\n",
    "import copy\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Computes values for plane visualization')\n",
    "parser.add_argument('--dir', type=str, default='/tmp/plane', metavar='DIR',\n",
    "                    help='training directory (default: /tmp/plane)')\n",
    "\n",
    "parser.add_argument('--grid_points', type=int, default=21, metavar='N',\n",
    "                    help='number of points in the grid (default: 21)')\n",
    "parser.add_argument('--margin_left', type=float, default=0.2, metavar='M',\n",
    "                    help='left margin (default: 0.2)')\n",
    "parser.add_argument('--margin_right', type=float, default=0.2, metavar='M',\n",
    "                    help='right margin (default: 0.2)')\n",
    "parser.add_argument('--margin_bottom', type=float, default=0.2, metavar='M',\n",
    "                    help='bottom margin (default: 0.)')\n",
    "parser.add_argument('--margin_top', type=float, default=0.2, metavar='M',\n",
    "                    help='top margin (default: 0.2)')\n",
    "\n",
    "parser.add_argument('--curve_points', type=int, default=61, metavar='N',\n",
    "                    help='number of points on the curve (default: 61)')\n",
    "\n",
    "parser.add_argument('--dataset', type=str, default='CIFAR10', metavar='DATASET',\n",
    "                    help='dataset name (default: CIFAR10)')\n",
    "parser.add_argument('--use_test', action='store_true',\n",
    "                    help='switches between validation and test set (default: validation)')\n",
    "parser.add_argument('--transform', type=str, default='VGG', metavar='TRANSFORM',\n",
    "                    help='transform name (default: VGG)')\n",
    "parser.add_argument('--data_path', type=str, default=None, metavar='PATH',\n",
    "                    help='path to datasets location (default: None)')\n",
    "parser.add_argument('--batch_size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size (default: 128)')\n",
    "parser.add_argument('--num_workers', type=int, default=4, metavar='N',\n",
    "                    help='number of workers (default: 4)')\n",
    "\n",
    "parser.add_argument('--model', type=str, default=None, metavar='MODEL',\n",
    "                    help='model name (default: None)')\n",
    "parser.add_argument('--curve', type=str, default=None, metavar='CURVE',\n",
    "                    help='curve type to use (default: None)')\n",
    "parser.add_argument('--num_bends', type=int, default=3, metavar='N',\n",
    "                    help='number of curve bends (default: 3)')\n",
    "\n",
    "parser.add_argument('--ckpt', type=str, default=None, metavar='CKPT',\n",
    "                    help='checkpoint to eval (default: None)')\n",
    "\n",
    "parser.add_argument('--wd', type=float, default=1e-4, metavar='WD',\n",
    "                    help='weight decay (default: 1e-4)')\n",
    "\n",
    "args = parser.parse_args(['--dir', 'plots/plot-normal-5354grad', '--data_path', 'data', '--model', 'VGG16', '--curve', \n",
    "                          'PolyChain', '--ckpt', 'points2plane/connect-normal-5556/checkpoint-100.pt'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plots/plot-normal-5354grad'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using train (45000) + validation (5000)\n",
      "Files already downloaded and verified\n",
      "Weight space dimensionality: 15245130\n",
      "Computing gradient vector\n",
      "0.020508204\n",
      "1.3164183\n",
      "6.10122\n",
      "6.648986\n",
      "3.4395576\n",
      "2.0869703\n",
      "1.4800283\n",
      "1.2656943\n",
      "1.402981\n",
      "1.9051266\n",
      "3.0564978\n",
      "5.6885357\n",
      "7.898111\n",
      "4.0841546\n",
      "0.05878834\n",
      "0.5266523679097493\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(args.dir, exist_ok=True)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "loaders, num_classes = data.loaders(\n",
    "    args.dataset,\n",
    "    args.data_path,\n",
    "    args.batch_size,\n",
    "    args.num_workers,\n",
    "    args.transform,\n",
    "    args.use_test,\n",
    "    shuffle_train=False\n",
    ")\n",
    "\n",
    "architecture = getattr(models, args.model)\n",
    "curve = getattr(curves, args.curve)\n",
    "\n",
    "curve_model = curves.CurveNet(\n",
    "    num_classes,\n",
    "    curve,\n",
    "    architecture.curve,\n",
    "    args.num_bends,\n",
    "    architecture_kwargs=architecture.kwargs,\n",
    ")\n",
    "curve_model.cuda()\n",
    "\n",
    "checkpoint = torch.load(args.ckpt)\n",
    "curve_model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "criterion = F.cross_entropy\n",
    "regularizer = utils.l2_regularizer(args.wd)\n",
    "\n",
    "\n",
    "def get_xy(point, origin, vector_x, vector_y):\n",
    "    return np.array([np.dot(point - origin, vector_x), np.dot(point - origin, vector_y)])\n",
    "\n",
    "\n",
    "w = list()\n",
    "curve_parameters = list(curve_model.net.parameters())\n",
    "for i in range(args.num_bends):\n",
    "    w.append(np.concatenate([\n",
    "        p.data.cpu().numpy().ravel() for p in curve_parameters[i::args.num_bends]\n",
    "    ]))\n",
    "\n",
    "print('Weight space dimensionality: %d' % w[0].shape[0])\n",
    "\n",
    "u = w[2] - w[0]\n",
    "dx = np.linalg.norm(u)\n",
    "u /= dx\n",
    "\n",
    "print('Computing gradient vector')\n",
    "\n",
    "def train(train_loader, model, optimizer, criterion, regularizer=None, lr_schedule=None):\n",
    "    loss_sum = 0.0\n",
    "    correct = 0.0\n",
    "\n",
    "    num_iters = len(train_loader)\n",
    "    model.train()\n",
    "    for iter, (input, target) in enumerate(train_loader):\n",
    "        if lr_schedule is not None:\n",
    "            lr = lr_schedule(iter / num_iters)\n",
    "            utils.adjust_learning_rate(optimizer, lr)\n",
    "        input = input.cuda(non_blocking=True)\n",
    "        target = target.cuda(non_blocking=True)\n",
    "\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        if regularizer is not None:\n",
    "            loss += regularizer(model)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        loss_sum += loss.item() * input.size(0)\n",
    "        pred = output.data.argmax(1, keepdim=True)\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "    grad = np.concatenate([p.grad.data.cpu().numpy().ravel() for p in model.parameters()])\n",
    "\n",
    "    return grad\n",
    "\n",
    "model1 = architecture.base(num_classes=10, **architecture.kwargs)\n",
    "model2 = architecture.base(num_classes=10, **architecture.kwargs)\n",
    "model3 = architecture.base(num_classes=10, **architecture.kwargs)\n",
    "model3.cuda()\n",
    "\n",
    "\n",
    "def init_model(p, model):\n",
    "    offset = 0\n",
    "    for parameter in model.parameters():\n",
    "        size = np.prod(parameter.size())\n",
    "        value = p[offset:offset + size].reshape(parameter.size())\n",
    "        parameter.data.copy_(torch.from_numpy(value))\n",
    "        offset += size\n",
    "\n",
    "\n",
    "init_model(w[0], model1)\n",
    "init_model(w[2], model2)\n",
    "\n",
    "criterion = F.cross_entropy\n",
    "regularizer = None\n",
    "optimizer = torch.optim.SGD(\n",
    "    filter(lambda param: param.requires_grad, model3.parameters()),\n",
    "    lr=0.01,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0\n",
    ")\n",
    "\n",
    "num_points = 15\n",
    "l_grad = []\n",
    "for j in range(0, num_points):\n",
    "    for p1, p2, p3 in zip(model1.parameters(), model2.parameters(), model3.parameters()):\n",
    "        alpha = j * 1.0 / (num_points-1)\n",
    "        p3.data.copy_(alpha * p1.data + (1.0 - alpha) * p2.data)\n",
    "    grad = train(loaders['train'], model3, optimizer, criterion, )\n",
    "    l_grad.append(grad)\n",
    "\n",
    "for i, v in enumerate(l_grad):\n",
    "    print(np.linalg.norm(v))\n",
    "    l_grad[i] -= np.dot(u, v) * u\n",
    "    l_grad[i] = v/np.linalg.norm(v)\n",
    "\n",
    "s = 0\n",
    "for v in l_grad:\n",
    "    s += v\n",
    "print(np.linalg.norm(s)/num_points)\n",
    "s = s/np.linalg.norm(s)\n",
    "\n",
    "\n",
    "v = w[1] - w[0]\n",
    "v -= np.dot(u, v) * u\n",
    "dy = np.linalg.norm(v)\n",
    "v /= dy\n",
    "\n",
    "# u2, v2 = copy.deepcopy(u), copy.deepcopy(v)\n",
    "# u1, v1 = copy.deepcopy(u), copy.deepcopy(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.dot(u2, u1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.dot(v2, v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.029102946\n",
      "0 -0.009224109\n",
      "1 -0.037704706\n",
      "2 -0.0581911\n",
      "3 -0.04219137\n",
      "4 -0.0033210842\n",
      "5 0.010927529\n",
      "6 0.013926087\n",
      "7 0.013118408\n",
      "8 0.013774304\n",
      "9 0.011390868\n",
      "10 -0.0018395763\n",
      "11 -0.047116976\n",
      "12 -0.062830776\n",
      "13 -0.019998532\n",
      "14 -0.010626069\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(s, v))\n",
    "for i, vec in enumerate(l_grad):\n",
    "    print(i, np.dot(v, vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5295691808064779\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(l_grad):\n",
    "#     print(np.linalg.norm(v))\n",
    "    l_grad[i] -= np.dot(u, v) * u\n",
    "    l_grad[i] = v/np.linalg.norm(v)\n",
    "\n",
    "s = 0\n",
    "for v in l_grad:\n",
    "    s += v\n",
    "print(np.linalg.norm(s)/num_points)\n",
    "s = s/np.linalg.norm(s)\n",
    "\n",
    "\n",
    "v = w[1] - w[0]\n",
    "v -= np.dot(u, v) * u\n",
    "dy = np.linalg.norm(v)\n",
    "v /= dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.027354201"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(s, v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.0037905849\n",
      "1 -0.024135474\n",
      "2 -0.057654627\n",
      "3 -0.04594074\n",
      "4 -0.0017186748\n",
      "5 0.013093248\n",
      "6 0.014068341\n",
      "7 0.013736997\n",
      "8 0.01338212\n",
      "9 0.011975017\n",
      "10 -0.0037048897\n",
      "11 -0.04647499\n",
      "12 -0.06424836\n",
      "13 -0.02056173\n",
      "14 -0.015314849\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(s, v))\n",
    "for i, vec in enumerate(l_grad):\n",
    "#     print(np.linalg.norm(vec))\n",
    "    print(i, np.dot(v, vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
